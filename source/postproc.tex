\section{Accelerating data analytics [2 pages, Paul]}\label{sec:postproc}

With the preparations for exascale computing, simulations with both higher resolution and frequency will be achievable, and the data production rates from these simulations will grow proportionally with the compute power of the machines.  The post-processing and analysis of these large datasets will require parallel technologies, and the previously existing technologies are mostly serial.  

While parallel technologies exist, their use would significant require changes to the post-processing workflow that currently exists.  We choose to take an approach that introduces the fewest changes to the workflow with which scientists are already familiar, namely parallelizing individual steps in the workflow by swapping out the serial scripts with parallel alternatives.  This approach will require less user re-education and should improve the adoption rate of the newly developed and adopted technologies.  This should also lead to fewer problems encountered during and after deployment of the new technologies.

We have chosen develop our new tools with parallel Python, using \mpipy\ for parallelism.  Python is excellent for rapid development, reducing the time to deployment.  It also allows us to develop tools with a very small code-base, making it easier to maintain.  Python seamlessly integrates into the existing script-driven CESM workflow, and is cross-platform, reducing the need to address build issues on multiple exascale platforms.

The general CESM post-processing workflow proceeds from the generation of the raw simulation data in the following way.  First, since post-processing and analysis tends to be performed on time-series data of only a handful of variables, the data must be transformed from files storing synoptic data (time-slice) to files storing a single variable over a long period of simulated time (time-series).  This reduces the amount of data moved when scientists copy datasets for local analysis, and it makes time-series data contiguous within a single file, improving I/O performance with conventional spinning disk storage.  The serial tool for performing this transformation is a script written using the NetCDF Operators (NCO), and this serial transformation step has been observed to take as long as the data generation with CESM itself.  The parallel Python tool that we have written to replace this NCO script is called the \pyreshaper.

The \pyreshaper\ implements a task-parallel approach to speeding up this data transformation; Each output file is defined and written in a single MPI process.  In particular, for the time-slice to time-series transformation, this means one output file for each time-series variable in the dataset.  This, obviously, has limited scalability, but the immediate benefit is obvious for large datasets which quite often have more than a few dozen time-series variables.  

The next step in the workflow is an analysis step, from the time-series files generate with the \pyreshaper.  In this step, temporal and spatial averages are computed for variables in the time-series dataset.  These averages, called climatologies, are primarily used to create plots that help scientists quickly assess the correctness of various aspects of the CESM component models.  The original script used to perform this task was written using NCO, and the new parallel Python tool that we have written to replace it is called the \pyaverager.

The \pyaverager\ implements a parallel approach based on the \pyreshaper, namely task-parallel with parallelization over the output files.  However, since the computation of the averages is a data reduction, it is possible to add an additional level of parallelization over the reduction steps, themselves.  Therefore, unlike the \pyreshaper\ which dedicates a single MPI process to each output file, the \pyaverager\ dedicates an MPI subcommunicator to each output file, allowing separate MPI processes to be independently responsible for reading and operating on the data.  This allows for greater scalability than the \pyreshaper\ can achieve, but still limited by the dataset and the kind and number of averages requested.

Both tools were tested in parallel on NCAR's Yellowstone supercomputing platform, located at the NCAR-Wyoming Supercomputing Center (NWSC) in Cheyenne, WY.  Two complete CESM datasets were used in these tests: a low-resolution (1-degree, 232 GB) dataset and a high-resolution (\onequarter-degree atmosphere/land, \onetenth-degree ocean/ice, 4.4 TB) dataset, both spanning 10 years of data.  At maximum parallelism, the \pyreshaper\ reduced the time-slice to time-series transformation step on the low- and high-resolution datasets from 3.7 hours and 23 hours, respectively, to 30 minutes and 2 hours, respectively.  That amounts to a $7.6\times$ and $11.7\times$ speedup, respectively.  The 
\pyaverager\ reduced the computation of climatologies for the low- and high-resolution datasets from 9 hours and 40 hours, respectively, to 4 minutes and 16 minutes, respectively.  That amounts to a $130\times$ and $150\times$ speedup, respectively.

Current and future development has been directed toward the application of these parallel Python techniques to more general and sophisticated problems, including the problem of data standardization and publication.  During model intercomparison projects, such as the Climate Model Intercomparison Project (CMIP), data generated by many modeling codes and institutions must be collected together for direct comparison with each other.  This necessitates data standardization, including conventions for variable naming, units, and additional metadata.  The process of standardization involves much that same procedures as those involved in the \pyreshaper\ and \pyaverager\ utilities, and these standardizations can even be seen as a generalization of the same tools.  This tool, in development at the time of this writing, is called \pyconform.

\pyconform\ is a general tool for performing dependent tasks, in a proscribed sequence, on large datasets to produce second datasets of a different format.  Such sequence of tasks comprise a directed acyclic graph (DAG), the flow of which describes the flow of data from input dataset to output dataset.  Obvious parallelism can be achieved across independent components of the DAG, while greater parallelism can be achieved by further splitting components of the DAG and connecting the components with MPI communication.  Further development is planned, and a release is expected before the end of 2016.
