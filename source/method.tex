
% The chapter may provide an outline of the basic scientific methodology, which can include a
% description of the fundamental equations or models as well as the type of scientific challenges
% that require using these methods on exascale resources. It is of particular interest to describe
% the formal complexity or method scalability with the size of the science problem. For some
% methods physical size of a system is the primary challenge, especially when the method scales
% non-linearly. Other problems may depend on being able to sample over significantly longer time
% periods in order to properly describe realistic experimental situations. Another aspect is the data
% type used in the algorithms, such as particle based, grid based methods, or a combination of
% these. Some algorithms can be dealing with linear access to very structured data, others rely on
% random access to irregular data.


\section{Scientific Methodology}\label{sec:method}

[{\color{red} John/Chris: }:This may need expansion/clarification]

While adapting the CESM code base to exploit massive fine-grained parallelism is necessary, this task is non-trivial at best; a complete re-write of a large, complex, and actively developed code like CESM is simply not possible with the resources at hand.  Therefore, we take an incremental approach toward preparing CESM for exascale. We focus on costly code kernels, re-working each to better utilize current and future computing capabilities.   

Our methodology, which we detail in subsequent subsections, can be summarized as follows.  We first profile the CESM code and look for expensive kernels whose improvement would impact the overall runtime.  We then extract the kernel in question to more easily experiment with optimizations. Next we apply both generic and specialized optimizations. Finally the improved kernel is incorporated back into the CESM code base, and we verify that the resulting simulation output has not had a climate-changing impact.  We note that some optimizations have a significant impact in terms of speedup, while other modifications pale in comparison. We incorporate optimizations at both ends of the spectrum as the computing resources required to run climate simulations are so significant that even a seemingly minor improvement (e.g., one or two percent) can translate into non-trivially lower computing costs.

\subsection{Performance analysis}

Describe how we first profile code and look for issues.  Briefly describe tools used (extrae, vtune) - maybe an example plot? And what we are looking for.

{\color{red}
 \begin{itemize}
   \item {Bob Walkup's simple library} [Kerr 1 paragraph]
   \item {Bult-in timers} [Kerr 1/2 paragraph]
   \item {Extrae folding} [Kim 1 paragraph]
 \end{itemize}
}


\subsection{Kernel extraction}

We extract the problematic kernel {\color{red} Better/more transition}.  

\input kgen.tex

\subsection{Optimization}

 {\color{red} Write this section - anyone?}

Here mention that optimizations can be ``easy'' - i.e., removing elemental functions - or ``involved'' - i.e., re-writing loop orders.  Similarly the payoff can be major or minor (not necessarily proportional to work involved).
Some optimizations are innovation - some are things the compiler should do (give examples).  Some are generic (brief paragraph about Raj's list)


\subsection{Ensemble verification}

 {\color{red} Allison : make shorter}

\input pycect.tex
